{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT1zuxg8xm5j"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHgmWzGJx147"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\",split='test',streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmEAVjxAvS90"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\",split='train',streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuaArRFHLYLF"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i13m0YD4_fQH"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b85jiy4swIcE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhmg5rk8yLos"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fTBCD0m1UTi"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers==4.3.0 sentencepiece==0.1.95 nltk==3.5 protobuf==3.15.3 torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lMtMuFY21arE"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "mname = \"marefa-nlp/marefa-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(mname)\n",
        "model = MarianMTModel.from_pretrained(mname)\n",
        "#input = \"President Putin went to the presidential palace in the capital, Kiev\"\n",
        "\n",
        "#translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch([input], return_tensors=\"pt\"))\n",
        "#translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
        "#print(translated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poVgS0S95B2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2w8ChoXPyKDE"
      },
      "outputs": [],
      "source": [
        "new_train_dataset=[]\n",
        "count=0\n",
        "for item in train_dataset:\n",
        "   count+=1\n",
        "   if count<1000:\n",
        "     translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch([item['sentence']], return_tensors=\"pt\"))\n",
        "     translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
        "     new_train_dataset.append({'audio':item['audio'],'sentence':translated_text})\n",
        "   else:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_dataset"
      ],
      "metadata": {
        "id": "tzNq-iFl8Z08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WwKAG7USykEr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ListDataset(IterableDataset):\n",
        "    def __init__(self, data_list):\n",
        "        self.data_list = data_list\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.data_list)\n",
        "    def cast_column(self, column_name, data_type):\n",
        "        for item in self.data_list:\n",
        "            item[column_name] = data_type(item[column_name], sampling_rate=16000)\n",
        "        return self\n",
        "    def map(self, function):\n",
        "        self.data_list = list(map(function, self.data_list))\n",
        "        return self\n",
        "\n",
        "new_train_dataset = ListDataset(new_train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zl96wn7RBTJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4812d495-b526-44cb-fa83-2484277f56f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ListDataset at 0x7d68b270ef20>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "new_train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xoQ9NXFL4GDQ"
      },
      "outputs": [],
      "source": [
        "new_test_dataset=[]\n",
        "count=0\n",
        "for item in test_dataset:\n",
        "   count+=1\n",
        "   if count<250:\n",
        "     translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch([item['sentence']], return_tensors=\"pt\"))\n",
        "     translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
        "     new_test_dataset.append({'audio':item['audio'],'sentence':translated_text})\n",
        "   else:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wxxZl3twCdv"
      },
      "outputs": [],
      "source": [
        "new_test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WYaUGg9mCBV1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "from datasets import Value\n",
        "class ListDataset(IterableDataset):\n",
        "    def __init__(self, data_list):\n",
        "        self.data_list = data_list\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.data_list)\n",
        "    def cast_column(self, column_name, data_type):\n",
        "        for item in self.data_list:\n",
        "            item[column_name] = data_type(item[column_name], sampling_rate=16000)\n",
        "        return self\n",
        "\n",
        "    def map(self, function):\n",
        "        self.data_list = list(map(function, self.data_list))\n",
        "        return self\n",
        "\n",
        "new_test_dataset = ListDataset(new_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CDUy1FI4GVF"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8yON1Ju4XLR"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Arabic\", task=\"translate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dV59BVI-4GWg"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Arabic\", task=\"translate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivIku4zv4lB_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3z6rh1ku4lDY"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "class AudioWrapper:\n",
        "    def __init__(self, audio_data, sampling_rate):\n",
        "        self.audio_data = audio_data\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "new_train_dataset = new_train_dataset.cast_column(\"audio\", AudioWrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABtkjaPf4yLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEDO5-k0R1fH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Um7m6bov4yMq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "\n",
        "    raw_speech_data = audio.audio_data\n",
        "\n",
        "    # Check if raw_speech_data is a dictionary and access the specific key if needed\n",
        "    if isinstance(raw_speech_data, dict):\n",
        "        raw_speech_data = raw_speech_data.get(\"array\", None)\n",
        "\n",
        "    # Check if raw_speech_data is a valid numeric type\n",
        "    if isinstance(raw_speech_data, (int, float, np.number, np.ndarray)):\n",
        "        # compute log-Mel input features from input audio array\n",
        "        input_features = feature_extractor(raw_speech_data, sampling_rate=audio.sampling_rate).input_features[0]\n",
        "\n",
        "\n",
        "        encoded_text = tokenizer(\n",
        "            batch[\"sentence\"],\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"np\",\n",
        "            max_length=10\n",
        "        )\n",
        "\n",
        "\n",
        "        labels = encoded_text.input_ids.flatten()\n",
        "\n",
        "        batch[\"input_features\"] = input_features\n",
        "        batch[\"labels\"] = labels\n",
        "    else:\n",
        "\n",
        "        print(\"Invalid raw speech data:\", raw_speech_data)\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ihXA5qUF4yQL"
      },
      "outputs": [],
      "source": [
        "new_train_dataset = new_train_dataset.map(prepare_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRqwmtV-R6b2"
      },
      "outputs": [],
      "source": [
        "#new_test_dataset = new_test_dataset.map(prepare_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QAygqZxN5E6u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0DsnL6Br5Ppt"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQZEkNGy0dqJ"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdtYRe150to9"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cKAneM3E5Ua5"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZCNz7TJz5UcW"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrQjFksF5f74"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqq8SZzu1D9J"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "15dUffeI5gua"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-ar\",\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WoPoKUef1tq7"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7t3btbQq5gv_"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=new_train_dataset,\n",
        "    eval_dataset=new_test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oL0gzCdn5uNa"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Kkxgbb0o5uPC",
        "outputId": "3dd1be0f-a8d2-4535-b890-304953243da0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='136' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 136/4000 08:11 < 3:56:25, 0.27 it/s, Epoch 2.00/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='140' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 140/4000 08:26 < 3:56:16, 0.27 it/s, Epoch 2.00/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVzW8z9McpR3"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import Audio, load_dataset\n",
        "\n",
        "# load model and processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"arabic\", task=\"translate\")\n",
        "ds = load_dataset(\"common_voice\", \"en\", split=\"test\", streaming=True)\n",
        "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "input_speech = next(iter(ds))[\"audio\"]\n",
        "input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
        "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
        "\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}